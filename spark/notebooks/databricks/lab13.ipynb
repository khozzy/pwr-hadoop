{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "787a3407-d677-42a8-9fc1-8f2553b20e91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Hurtownie Danych i Big Data\n",
    "## Laboratorium 13 - Uruchomienie przykładowych programów dla danych masywnych\n",
    "\n",
    "Celem laboratoriów jest zapoznanie się z **koncepcją RDD** oraz podstawowymi rodzajami operacjami - _transformacjami_ i _akcjami_.\n",
    "\n",
    "### Agenda\n",
    "1. Czym jest RDD?\n",
    "2. Tworzenie RDD (`parallelize()`, `textFile()`)\n",
    "3. Podstawowe operacje (`filter()`, `map()`, `collect()`)\n",
    "4. Próbkowanie (`sample()`, `takeSample()`)\n",
    "5. Operacje na zbiorach (`union()`, `intersect()`, `subtract()`, `distinct()`, `cartesian()`)\n",
    "6. Agregacje (`reduce()`, `fold()`, `aggregate()`)\n",
    "7. Operacje klucz/wartość\n",
    "\n",
    "### 1. Czym jest RDD?\n",
    "_Resilient Distributed Datasets_ (RDD) są podstawową abstrakcją służącą do przechowywania danych we frameworku Apache Spark. Ich wykorzystanie gwarantuje przede wszystkim poprawność obsługiwania błędów (ang. _fault tolerant_) w przypadku awarii lub błędów jednego z węzłów. W praktyce wejściowy zbiór danych dzielony jest na mniejsze partycje, z których każda jest stała i niemodyfikowalna (ang. _immutable_). Spark pamięta kolejność wykonywania poszczególnych operacji na wejściowym zbiorze danych i jest w stanie dynamicznie odtworzyć każdą partycję danych (tworząc w pamięci graf operacji, tzw. _DAG_).\n",
    "\n",
    "#### Dotychczasowe podejście - Hadoop MapReduce\n",
    "MapReduce jest popularnym podejściem do radzenia sobie z przetwarzaniem dużej ilości danych. Z biegiem czasu powstało wiele wzorców projektowych usprawniających cały proces. Największym **problemem jest jednak sekwencyjne łączenie kolejnych zadań**, np. filtrowanie -> analiza, (tzw. *job-chaining*), które jest mało efektywne.\n",
    "\n",
    "Wąskim gardłem okazują się być szczególnie:\n",
    "- odczyt/zapis danych na dysku `I/O` (Hadoop po każdym zadaniu `\"job\"` zapisuje dane w zewnętrznym systemie plików (zazwyczaj jest to HDFS). Następnie, przy kolejnym zadaniu te dane są ponownie odczytywane co jest dosyć kosztowe).\n",
    "- transfer/replikacja danych na inne węzły (`network I/O`),\n",
    "- serializacja obiektów (`CPU`)\n",
    "\n",
    "> Szacuje się, że ok **90%** czasu zadania przeznaczane jest na operacje I/O.\n",
    "\n",
    "![Iterative operations on MapReduce](https://www.tutorialspoint.com/apache_spark/images/iterative_operations_on_mapreduce.jpg)\n",
    "\n",
    "#### Jak Spark może pomóc?\n",
    "Apache Spark rozwiązuje ten problem wykorzystujac koncepcję przechowywanie danych między poszczególnymi zadaniami w pamięci RAM poszczególnych maszyn. W ten sposób możliwe jest **przyspieszenie operacji 10-100x** (zredukowane użycie dysku oraz sieci).\n",
    "\n",
    "> Dopiero w przypadku wyczerpania pamięci RAM dane będą zapisywane na dysku (domyślne ustawienie).\n",
    "\n",
    "![Iterative operations on Apache Spark](https://www.tutorialspoint.com/apache_spark/images/iterative_operations_on_spark_rdd.jpg)\n",
    "\n",
    "#### Cechy RDD\n",
    "- niemodyfikowalna (`immutable` / `read-only`) kolekcja dowolnych obiektów (pożądana cecha w przypadku rozproszonego przetwarzania danych, skutecznie redukuje ryzyko związane z niespójnością danych),\n",
    "- dzielenie kolekcji na części (`partitioning`) i ich efektywna dystrybucja w klastrze (rozbudowane możliwości personalizacji),\n",
    "- odporność na błędy (`fault-tolerant`) - zapamiętanie sekwencji operacji (DAG) umożliwiające późniejsze odtworzenie stanu dowolnej partycji\n",
    "\n",
    "### 2. Tworzenie RDD\n",
    "\n",
    "RDD można utworzyć na 2 sposoby:\n",
    "- na podstawie aktualnej kolekcji (metoda `parallelize()`) - w sterowniku aplikacji (`driver`),\n",
    "- na podstawie zewnętrznych danych (np. HDFS, S3, HBase, ...)\n",
    "\n",
    "Poniżej przedstawiono przykład pierwszego sposobu.\n",
    "\n",
    "Zdefiniujmy listę (podstawowa kolekcja danych w Python-ie) ze 100 elementami będącymi kolejnymi liczbami naturalnymi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f4da1e2d-2b90-47bb-99ec-dc58cf70a623",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = range(0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "96a45786-964d-45c8-8ed5-0006a919fed3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Transformacja do RDD odbywa się za pomocą funkcji `parallelize()` wykonanej na obiekcie kontekstu - `sc`. Ponieważ bezpośrednie odwołanie do tego obiektu jest odradzane (zaszłość z wcześniejszych wersji frameworka utrzymywana w celu zapewnienia wstecznej kompatybilności), skorzystamy ze zmiennej `spark` która agreguje w sobie ww. kontekst jako atrybut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6933c54b-9d25-4952-b536-b62a629f94e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Poniższe wywołanie jest także poprawne jednak odradzane\n",
    "# a_rdd = sc.parallelize(a)\n",
    "\n",
    "a_rdd = spark.sparkContext.parallelize(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "da66d6e6-98e6-47de-a1ba-26e40a5512fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Następnie możemy wywołać naszą pierwszą **akcję** - `count()` - zwracającą ilości elementów w kolekcji.\n",
    "\n",
    "> Wywołanie **akcji** sygnalizuje rozpoczęcie procesu przetwarzania danych. Do tego momentu wszystkie **operacje** konstruują graf przetwarzania danych - ale **nie uruchamiają** faktycznych obliczeń. Dopiero wywołanie akcji w sterowniku daje sygnał do rozpoczęcia przetwarzania, wysłania danych na osobne maszyny, a następnie ich wtórną agregację."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e9095c80-5c5f-42d9-bee5-27d7fc8effc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wywołanie akcji\n",
    "a_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "68153a92-1fbd-445f-b5ba-bd977df9b6b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Metoda `count` jest jedną z wielu innych dostępnych akcji. Ten [link](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions) zawiera odnośnik do dokumentacji przedstawiającej wszystkie dostępne metody w aktualnej wersji.\n",
    "\n",
    "Framework liczy elementy dla każdej partycji zlokalizowanej na każdym węźle osobno, a następnie przesyła cząstkowe wyniki do sterownika, gdzie są one ostatecznie sumowane w celu podania ostatecznego wyniku.\n",
    "\n",
    "Innym przykładem akcji jest metoda `collect()`. Przesyła ona elementy przetworzone elementy RDD z powrotem do sterownika.\n",
    "\n",
    "W naszym przykładzie dane są małe i jej wywołanie nie stanowi problemu. Jednak w przypadku operacji na \"dużych danych\", przekierowanie takiej ilości do jednej maszyny prawdopodobnie nie jest dobrym pomysłem. Należy wcześniej oszczacować ilość przesłanych danych - inaczej prawdopodobnie przepełnimy pamięć sterownika.\n",
    "\n",
    "Lepszym sposobem jest np. przesłanie kilku pierwszych elementów korzystając z akcji `take(num)`, gdzie `num` jest liczbą próbek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c7af30c1-3161-4998-96a7-ede845a5363f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Należy być ostrożnym. `collect()` zwróci wszystkie elementy\n",
    "# a_rdd.collect()\n",
    "\n",
    "# Natomiast `take(5)` zwróci 5 pierwszych elementów z RDD \n",
    "a_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "75207c1c-3f1b-4b76-8ced-6e66276378c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# W przypadku losowych próbek możemy wykorzystać `takeSample()`, gdzie pierwszy argument określa czy wylosowana liczba może trafić z powrotem do puli\n",
    "\n",
    "a_rdd.takeSample(False, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f17b0df0-4ccd-40be-bf9b-1320064b29c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Załadujmy teraz nieco ciekawszy zbiór danych - *KDD Cup 1999*. Opisuje on około pół miliona sieciowych interakcji podzielonych na *\"normalny\"* oraz *\"podejrzany\"* ruch. Dokładny opis poszczególnych pól znajduje się [tutaj](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99).\n",
    "\n",
    "Jest to typowy plik CSV, który po rozpakowaniu zajmuje ok. 75 MB. Każda linia pliku zawiera osobny wiersz danych odseparowanych znakiem `,`.\n",
    "\n",
    "Załadujmy go jako RDD wykorzystując metodę `textFile()`.\n",
    "\n",
    "> Spark automatycznie radzi sobie ze skompresowanymi plikami. Jednak w tym przypadku zbiór danych nie jest dzielony na partycje (używana jest 1 partycja, więc dane nie będą przetwarzane na wielu maszynach). Dobrą praktyką jest jawne zadeklarowanie partycjonowania - np. poprzez wywołanie operacji `repartition()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d77e8537-1ad5-4b1d-856a-5d011798c6d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nalezy zmienić tą linię w zależności od lokalizacji pliku z danymi\n",
    "local_file_path = '/FileStore/tables/kddcup_data_10_percent-d8e1d.gz'\n",
    "\n",
    "# Tworzenie RDD z pliku\n",
    "raw_data_rdd = sc.textFile(local_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d51494bc-12dc-4429-865a-d595248ebfbd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Sprawdźmy czy dane zostały poprawnie załadowane wyświetlając całkowitą liczbę rekordów (w tym przypadku linii)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b5de74d7-ac2e-4bd3-8b5b-af98d8e92a13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wywołanie akcji `count`\n",
    "raw_data_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9b88acad-31f5-48af-892c-4e2be9b2086b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Sprawdzmy czy faktycznie, dane nie są podzielone na partycje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ccea9093-155b-41e8-87bc-6367acc65fda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# `getNumPartitions()` zwraca liczbę partycji na którą podzielone są dane w RDD\n",
    "raw_data_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "92396698-dc76-4e8c-bb6b-1a0fd2c4f616",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Zwiększymy liczbę partycji do 5. Aby dowiedzieć się więcej jak działa mechanizm dzielenia danych w RDD na partycje zachęcam do zapoznania się z moim publicznym [artykułem](https://medium.com/parrot-prediction/partitioning-in-apache-spark-8134ad840b0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2c4bedba-c55a-4888-a8e1-3eeb266d3f33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_data_rdd = raw_data_rdd.repartition(5)\n",
    "raw_data_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e6d133ec-3ec4-40ec-8883-edd686bdf7df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Podstawowe operacje\n",
    "Zobaczmy jak wykorzystać 3 podstawowe operacje - `filter()` (transformacja), `map()` (transformacja) oraz `collect()` (akcja).\n",
    "\n",
    "Zanim przejdziemy do wykonywania operacji dobrą praktyką jest zapoznanie się z danymi.\n",
    "\n",
    "Rzućmy okiem na pierwsze dwa rekordy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4a347665-6cfe-441e-aeb0-1ef544e8c77d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_data_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d220f6fa-22b8-4ac5-a484-afebaaa1d2e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Wynikiem jest tablica zawierająca 2 elementy które są ciągami znaków, większość danych jest numeryczna, ostatnie pole określa typ interakcji (tutaj jest to `normal`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7845f0f9-5e0e-4f8b-87d3-6647645a8eda",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Transformacja `filter()`\n",
    "Transformacja `filter()` wykonuje zdefiniowaną _funkcję_ określającą czy dany element spełnia określone warunki zwracając nowy RDD.\n",
    "\n",
    "Niżej przedstawiono przykład pobrania jedynie wierszy zawierających słowo \"*normal*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "abee17de-23c4-4a2f-af9f-4730b98b8439",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Funkcja zwracająca `True` jeśli w danym elemencie znajduje się ciąg znaków \"normal\"\n",
    "def is_normal_interaction(row):\n",
    "  return 'normal' in row\n",
    "\n",
    "normal_raw_data_rdd = raw_data_rdd.filter(is_normal_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f380bee3-b13b-4a3e-b6f9-0059ac6bec78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Python umożliwia zapisanie tej samej funkcji w skondensowanej formie wykorzystując tzw. [wyrażenia lambda](http://stackoverflow.com/questions/890128/why-are-python-lambdas-useful) (anonimowa funkcja). W przypadku krótkich operacji jest to dobra metoda do zwiększenia przejszystości kodu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aec58a8b-24ed-42d7-b8da-67320d4e1baf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analogiczny kod, jednak bardziej czytelny\n",
    "normal_raw_data_rdd = raw_data_rdd.filter(lambda row: 'normal' in row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ce3132bf-e87c-4e68-ae29-1ada4c850ccc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Istotnym w tym przypadku jest to, że samo wywołanie funkcji, która jest _transformacją_ **nie powoduje** rozpoczęcia żadnych obliczeń. Spark dopisuje żądanie wykonania danej operacji na danej partycji, a następnie czeka do momentu wywołania pierwszej akcji (która uruchamia cały ciąg przetwarzania).\n",
    "\n",
    "Możemy łatwo to zrobić wywołując wcześniej poznaną akcję `count()` zliczającą na świeżo utworzonym RDD `normal_raw_data_rdd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "81536203-386e-461c-93c0-66021fa28bf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dopiero wywołanie akcji powoduje wykonanie kodu odpowiedzialnego za filtrowanie\n",
    "normal_raw_data_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "440efbb2-648a-4cee-8ede-1d77c456c153",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Liczba zwróconych rekordów jest mniejsza niż tych w początkowym RDD, co świadczy o tym że funkcja została faktycznie wykonana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "73453518-c36f-4167-b9e5-e0833e0fa0a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Transformacja `map()`\n",
    "Transformacja `map()` wykonuje zadaną funkcję na każdym elemencie RDD zwracając nowy RDD. Funkcja ta może wykonywać dowolną operację na elemencie przekształcając go wedle uznania.\n",
    "\n",
    "> Np. jeśli RDD przechowuje liczby funkcja może mnożyć każdą z nich przez określoną stałą, jeśli przechowywane są ciągi znaków możemy transformować je do małych liter.\n",
    "\n",
    "W naszym przypadku każdy wiersz danych reprezentowany jest jako ciąg danych odseparowany znakiem `,` (czyli standardowy plik CSV). Wywołamy na nim standardową funkcję `split()` która podzieli go na części (na podstawie danego znaku) tworząc _listę_, gdzie każda komórka jest osobnym polem.\n",
    "\n",
    "> Co chcemy uzyskać: `'a,b,c' => ['a', 'b', 'c']`\n",
    "\n",
    "Tak przetworzone dane będą łatwiej dostępne w późniejszych obliczeniach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "03173f94-6666-4c35-a6fa-21fbfaf2c500",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zadeklarowanie transformacji `map()` na wszystkich danych\n",
    "csv_data_rdd = raw_data_rdd.map(lambda row: row.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0434e070-b97c-41d2-b87f-8166aadbec85",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Zweryfikujmy poprawność operacji badając pierwszy element - akcja `first()` (analogiczna do `take(1)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ef36401c-db19-4362-b07e-2a43b6101191",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(csv_data_rdd.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cdd12e95-3e72-4d93-8a33-5f46fe92a53d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Jak widać cel został osiągnięty. Wciąż nie jest to idealne rozwiązanie, ponieważ każde pole jest reprezentowane jako ciąg znaków. Docelowo pewnie chcielibyśmy rozpoznawać typy (np. liczby całkowite czy zmiennoprzecinkowe). \n",
    "\n",
    "Poniżej zdefiniujemy inne mapowanie. Będzie ono zwracało  \"krotkę\" (*tuple*) w postaci `<klucz, wartość>`. Klucz określa typ danej interakcji (normalna lub złośliwa), drugi oryginalny rekord. Badając dane wiemy, że typ interakcji zawsze występuje na tej samej pozycji (41 element).\n",
    "\n",
    "> Reprezentacja elementów RDD w postacji `<klucz, wartość>`, jest bardzo popularna i umożliwia dostęp do szerokiej dedykowanej puli osobnych operacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8402694a-22a4-4496-9dd9-29e6f3162b3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def parse_interaction(line):\n",
    "  elems = line.split(',') # podział wiersza na tablicę\n",
    "  key = elems[41] # rodzaj interakcji, zawsze na tej pozycji\n",
    "  vals = elems[:40] # reszta danych\n",
    "  return (key, vals) # zwracamy dwu-elementową krotkę\n",
    "\n",
    "csv_data_2_rdd = raw_data_rdd.map(parse_interaction)\n",
    "\n",
    "print(csv_data_2_rdd.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "45fd09e1-f9d2-40ae-a098-0f41904695c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Akcja `collect()`\n",
    "Dotychczas używaliśmy jedynie akcji `count()`, `take()` i `first()`\n",
    "\n",
    "> Wywołanie funkcji `collect()` sprawi pobranie **wszystkich** elementów RDD do pamięci sterownika (`driver`). **Należy używać tej funkcji ze szczególną ostrożnością**.\n",
    "\n",
    "Jej użycie powinno być wykonane w momencie zbieranie finalnych danych statystycznych, które nie będą miały znaczącego wpływu na wydajność maszyny obsługującą sterownik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "57ea718d-56a8-43c5-89a0-9f68514b2a79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pobranie wszystkich danych do lokalnej zmiennej\n",
    "local_csv = csv_data_2_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba0f6235-3e97-42d7-b9f0-bfdba1051697",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Możemy sprawdzić typ otrzymanej zmiennej i kilka przykładowych wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "629d04e7-7b45-4b16-b167-7d7f17a454c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print('Typ zmiennej: {}'.format(type(local_csv)))\n",
    "print('Ilość elementów: {}'.format(len(local_csv)))\n",
    "print('Rozmiar danych: ~{} MB'.format(sys.getsizeof(local_csv) / 1048576))\n",
    "print('Typ pojedynczego elementu: {}'.format(type(local_csv[0])))\n",
    "print('Klucz pierwszego elementu: {}'.format(local_csv[0][0]))\n",
    "print('Wartość pierwszego elementu: {}'.format(local_csv[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e606a013-4a4e-4998-92ab-b2887fbf88a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Próbkowanie\n",
    "Próbkowanie jest operacją zwracającą losowe elementy ze zbioru danych z określonym prawdopodobieństwem. Przydaje się szczególnie podczas szybkiej analizy dużych zbiorów danych, gdzie np. 10% wszystkich danych jest w stanie reprezentować całość populacji z dobrym przybliżeniem.\n",
    "\n",
    "Spark udostępnia 2 metody dla próbkowania - transformację `sample()` i akcję `takeSample()`. Wywołanie akcji skutkuje oczywiście pobraniem danych do aktualnej maszyny (jak w przypadku `collect()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b550d9e4-4f4b-4822-a11e-bd60fc415de5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Transformacje `sample()`\n",
    "Poniższy eksperyment polega na wybraniu 10% wszystkich elementów i sprawdzniu jaka część wierszy reprezentuje poprawną transmisję danych.\n",
    "\n",
    "Na początku zdefiniujemy funkcję która dla danego RDD obliczy dwie wartości - liczbę wszystkich interakcji i liczbę normalnych interakcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "26d4f7de-d29c-4662-8535-d0e132692d47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def count_interactions(rdd):\n",
    "  total_count = rdd.count()\n",
    "  normal_count = rdd.filter(lambda x: 'normal' in x).count()\n",
    "  return total_count, normal_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dc56e0cc-4634-4263-b16d-29de93cc2849",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "W przypadku transformacji `sample()` należy podać 3 parametry:\n",
    "1. czy próbkować ze zwracaniem elementów z powrotem do zbioru (typ `bool`),\n",
    "2. jaką część oryginalnego zbioru chcemy otrzymać (typ `float`),\n",
    "3. liczbę na podstawie której zainicjalizowany będzie generator liczb pseudolosowych (tzw. _seed_, typ `int`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1ff87c67-00c8-4e28-a3dc-59225c7dbe73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SAMPLE_PORTION = 0.1\n",
    "SEED = 123\n",
    "\n",
    "# Tworzymy nowy RDD jedynie z części danych\n",
    "sample_raw_data_rdd = raw_data_rdd.sample(False, SAMPLE_PORTION, SEED)\n",
    "\n",
    "# Wywołanie funkcji `count_interactions()`\n",
    "sampled_count, sampled_normal_count = count_interactions(sample_raw_data_rdd)\n",
    "\n",
    "print(\"{} / {} = {:.2f}% normalnych interakcji w zbiorze danych\".format(sampled_normal_count, sampled_count, sampled_normal_count*100.0/sampled_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "18aabc33-020d-4695-ab9b-e45ebccbcca7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Zobaczmy jak wyglądają te wartości dla całej populacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8a8617c7-3aca-4d8e-91da-0c61e36cf48e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wywołanie funkcji `count_interactions()`\n",
    "all_count, all_normal_count = count_interactions(raw_data_rdd)\n",
    "\n",
    "print(\"{} / {} = {:.2f}% normalnych interakcji w zbiorze danych\".format(all_normal_count, all_count, all_normal_count*100.0/all_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4273a98e-505d-445d-8367-4455a4ed9eb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "W tym przypadku wartości są bardzo zbliżone a operacje wykonywane były na 10x mniejszym zbiorze.\n",
    "\n",
    "#### Akcja `takeSample()`\n",
    "Akcja `takeSample()` pobiera wyznaczoną ilość rekordów do sterownika. Jest też często uznawana za bezpieczniejszy wariant `collect()` ponieważ możemy z góry oszacować jak dużo danych zostanie przesłanych.\n",
    "\n",
    "Należy podać 3 parametry:\n",
    "1. czy próbkować ze zwracaniem elementów z powrotem do zbioru (typ `bool`),\n",
    "2. ile losowych elementów ma zostać zwróconych (typ `int`),\n",
    "3. liczbę na podstawie której zainicjalizowany będzie generator liczb pseudolosowych (typ `int`)\n",
    "\n",
    "Przykład niżej pobiera 5 losowych próbek z całego zbioru danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a41664c1-4b4c-4af2-8437-887c42b6c7ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand_raw_data = raw_data_rdd.takeSample(False, 5, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "05fa6fea-dc0e-456f-9112-633ea9b21f29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for row in rand_raw_data:\n",
    "  print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d7da98ca-75e8-428b-997e-5625d81c8dca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Operacje na zbiorach\n",
    "Spark umożliwia także proste operacje na zbiorach. Poniżej przedstawiono przykładowe wywołania w celu zrozumienia ogólnej idei. W celu stworzenie RDD używamy metody `parallelize()` przekazując jako argument tablicę elementów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cf1384a1-3639-4575-96f8-0a5a3d383b10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Suma - `union()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8d020008-c712-4c77-addf-b13f266e29f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd_1 = spark.sparkContext.parallelize(['A','B','C'])\n",
    "rdd_2 = spark.sparkContext.parallelize(['G','H','I'])\n",
    "\n",
    "spark.sparkContext.union([rdd_1, rdd_2]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad33267e-01cc-4d9c-aac3-6300d2b4efd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Część wspólna - `intersect()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b7cf93f6-2682-4b04-ac19-80c42651e41a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd_1 = spark.sparkContext.parallelize(['A','B','C'])\n",
    "rdd_2 = spark.sparkContext.parallelize(['G','B','C'])\n",
    "\n",
    "rdd_1.intersection(rdd_2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3f34f3d4-a577-46f7-a532-2351adfc8066",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Różnica - `subtract()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "620abf81-bc7d-4869-a241-ce34949c1f68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd_1 = spark.sparkContext.parallelize(['A','B','C','D','E'])\n",
    "rdd_2 = spark.sparkContext.parallelize(['A','B'])\n",
    "\n",
    "rdd_1.subtract(rdd_2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9cb9aff7-13f7-4af9-a298-2d2a7f2b3f64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Unikalne wartości - `distinct()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a9a7d339-f9f8-4cbd-9f79-5e62546c09a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd_1 = spark.sparkContext.parallelize(['A','B','C','A','C','B','A'])\n",
    "\n",
    "rdd_1.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1cac1515-3549-4c6f-ab88-658fcbe49ded",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Iloczyn kartezjański - `cartesian()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0398a98d-c799-4509-a23b-b60735a2f404",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd_1 = spark.sparkContext.parallelize(['A','B','C'])\n",
    "rdd_2 = spark.sparkContext.parallelize(['1','2', '3'])\n",
    "\n",
    "rdd_1.cartesian(rdd_2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "087bbf50-61e2-4a14-8ae2-8bf9f8b7a108",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 6. Agregacje\n",
    "Spark udostępnia 3 niskopoziomowe akcje służące agregowaniu danych numerycznych - `reduce()`, `fold()` i `aggregate()`\n",
    "\n",
    "#### Akcje `reduce()` i `fold()`\n",
    "Akcje `reduce()` i `fold()` są do siebie bardzo zbliżone. Obie pobierają jako argument funkcję agregującą dwa elementy. Akcja `fold()` pozwala dodatkowo określić wartość początkową dla skumulowanej operacji (w przypadku `reduce()` jest to domyślnie zero).\n",
    "\n",
    "Poniższy przykład zademonstruje działanie funkcji `reduce()` w celu określenia długości trwania wszystkich normalnych i podejrzanych interakcji.\n",
    "\n",
    "Na początek wyodrębnijmy dwa RDD - jeden zawierające normalne interakcje, drugi pozostałe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8c9bfc69-ed0d-4169-8d72-f85550307af2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wyodrębnienie osobnych RDD na podstawie wcześniej obliczonego RDD `csv_data`\n",
    "normal_data_rdd = csv_data_rdd.filter(lambda row: 'normal' in row[41])\n",
    "attack_data_rdd = csv_data_rdd.filter(lambda row: 'normal' not in row[41])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "adf099c6-86c2-4126-a9e9-87143b8bdcce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Zdefiniujmy funkcję pomocniczą która:\n",
    "1. wyłuska pierwszy pole z każdego elementu (to jest długość trwania interakcji) i przekształci je na typ liczbowy (rzutowanie `int()`),\n",
    "2. na tak uzyskanym numerycznym RDD wykonamy akcję `reduce()` które doda kolejne elementy do siebie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dc943e72-b8ce-412d-aae0-aaa4b7e65abb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def count_interaction_stats(rdd):\n",
    "  interactions_count = rdd.count() # liczba interakcji\n",
    "  total_interactions_duration = rdd.map(lambda row: int(row[0])).reduce(lambda x, y: x+y) # suma długości trwania wszystkich interakcji\n",
    "  return interactions_count, total_interactions_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "40612063-1272-4755-bf7b-681c07e24056",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Zobaczmy czy istnieje różnica pomiędzy średnią długością interakcji pomiędzy normalnymi i podejrzanymi zdarzeniami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "17d37e1a-1a7f-4575-bd05-db9def563f2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# zlecenie rozproszonych obliczeń\n",
    "normal_count, normal_total_duration = count_interaction_stats(normal_data_rdd)\n",
    "attack_count, attack_total_duration = count_interaction_stats(attack_data_rdd)\n",
    "\n",
    "# prezentacja wyników\n",
    "print(\"Średni czas normalnej interakcji:\\t{:.2f}\".format(normal_total_duration/normal_count))\n",
    "print(\"Średni czas podejrzanej interakcji:\\t{:.2f}\".format(attack_total_duration/attack_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7bbbaafa-8fab-4ea9-8d9c-dae9f96a1fea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Jak widać już wstępna i pobieżna analiza zachęca do dalszej eksploatacji danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ee0d5f72-f3c5-493a-bd1f-c0d53b3817f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Akcja `aggregate()`\n",
    "Innym sposobem jest użycie akcji `aggregate()` (która jest wykorzystywania niskopoziomowo zarówno przez `reduce()` jak i `fold()`). Różnica polega na tym, że posiadamy jeszcze większą kontrolę nad procesem.\n",
    "\n",
    "W przypadku funkcji `reduce()` oraz `fold()` mogliśmy decydować o tym jak łączone ze sobą były dwa elementy (ang. _combiner_).\n",
    "\n",
    "Funkcja `aggregate()` daje dodatkowo możliwość tworzenia tzw. \"_akumulatora_\" przechowującego lokalnie dane. Użytkownik musi  sprecyzować:\n",
    "- wartość początkową akumulatora (jak w przypadku `fold()`),\n",
    "- funkcję dodającą wartości do akumulatora,\n",
    "- funkcję dodającą do siebie dwa akumulatory\n",
    "\n",
    "Zasadniczą korzyścią jest to, że **ostatecznie zwracany typ elementów nie musi być tego samego typu co elementy w początkowym RDD**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dca30d86-c882-48ee-837a-bffd5dde1fc1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Poniższa funkcja realizuje ten sam problem co powyżej - obliczaja średni czas trwania poszczególnych typów interakcji.\n",
    "\n",
    "> Zwróć uwagę, że zwrócony typ danych jest dwu-elementową krotką (*tuple*) przechowującą informacje o:\n",
    "1. całkowitym czasie ataku\n",
    "2. ilości ataków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6ae6e2bd-198d-4478-b4eb-945b14ceecd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def count_interaction_stats_2(rdd):\n",
    "  duration, count = rdd.aggregate(\n",
    "    (0,0), # początkowa wartość akumulatora (sumaryczny czas trwania, ilość interakcji)\n",
    "    (lambda acc, row: (acc[0] + int(row[0]), acc[1] + 1)),  # dodanie danych do akumulatora\n",
    "    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))  # dodanie dwóch akumulatorów\n",
    "  )\n",
    "  return duration, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "13185dc4-423a-4a78-b8e1-e867aaf63018",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# przetworzenie danych\n",
    "normal_total_duration, normal_count = count_interaction_stats_2(normal_data_rdd)\n",
    "attack_total_duration, attack_count = count_interaction_stats_2(attack_data_rdd)\n",
    "\n",
    "# prezentacja wyników\n",
    "print(\"Średni czas normalnej interakcji:\\t{:.2f}\".format(normal_total_duration/normal_count))\n",
    "print(\"Średni czas podejrzanej interakcji:\\t{:.2f}\".format(attack_total_duration/attack_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "270b237b-56cf-4ccd-bb48-039b57d41637",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Jak widać wyniki są identyczne jak w przypadku funkcji `reduce()`. Co istotne wejściowy RDD przechowywał dane w formacie tekstowym, a rezultat operacji to krotka z dwoma wartościami numerycznymi. Udało nam się przenieść logikę odpowiedzialną za przekształcenie elementu, do funkcji dodającej dane do akumulatora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2f0349d0-70ee-4fc3-b037-e9d7ff19c9e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 7. Operacje klucz/wartość\n",
    "Szczególnym typem operacji są operacje typu *klucz/wartość*. W Pythonie realizowane są za pomocą **dwu-elementowych krotek**. Na pierwszej pozycji znajduje się klucz rekordu, na drugim wartość (dowolnego typu). Taka reprezentacja danych umożliwia logiczny podział danych i sprawniejsze przetwarzanie (w szczególności z użyciem partycjonowania).\n",
    "\n",
    "> Należy zwrócić uwagę na to aby dane były równomiernie rozmieszczone w kluczach (uniknięcie sytuacji kiedy jeden klucz zawiera 90% wszystkich danych).\n",
    "\n",
    "Aby reprezentować zbiór danych w takiej formie wystarczy zmapować go do krotki. W poniższym przykładzie użyjemy RDD `csv_data_rdd`, gdzie kluczem będzie typ interakcji, a wartością cały wiersz rekordu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c37da342-9eff-45bf-b5e5-5719fd2af18d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dane reprezentowane w postaci krotki - <klucz, wartość> => <interakcja, oryginalny wiersz>\n",
    "key_value_data_rdd = csv_data_rdd.map(lambda x: (x[41], x))\n",
    "key_value_data_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b50930d7-91ea-4e59-b38a-5abd0205fc96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Mając dane w tej postaci Spark udostępnia szerg metod specjalnie zoptymalizowanych pod ten typ danych, np. akcja `countByKey()` zlicza ilość wystąpień elementów każdego klucza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c23f3ae8-497e-4543-ac53-7a6b044918a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zliczenie elementów dla każdego klucza\n",
    "interaction_counts = key_value_data_rdd.countByKey()\n",
    "\n",
    "# prezentacja wyników\n",
    "for interaction, count in interaction_counts.items():\n",
    "  print(\"{} => {}\".format(interaction, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2e5af54e-062e-4dc4-aac5-e745ac1204f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Inną akcją jest `reduceByKey()` gdzie możemy wykonać określoną funkcję tylko na elementach z określonym kluczem.\n",
    "\n",
    "Poniższy przykład zlicza całkowity czas interakcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b68674a1-159a-4e3c-a4d6-dd1d826a8a38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dane mapowane do krotki (typ interakcji, czas interakcji)\n",
    "key_value_duration_rdd = csv_data_rdd.map(lambda row: (row[41], float(row[0])))\n",
    "\n",
    "# Redukcja względem każdego klucza\n",
    "durations_by_key_rdd = key_value_duration_rdd.reduceByKey(lambda x,y: x + y)\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "durations_by_key_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f230e714-2e6b-45bf-91f7-b4386ca5d634",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Inna funkcja agregująca - `combineByKey()` może zostać użyta do obliczenia krotki - czas trwania i ilość wystąpień dla każdego klucza. Działa ona na podobnej zasadzie co `aggregate()` z przykładu wyżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3c69dbd9-06f3-4144-bb7a-1e73bb65a9e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum_counts_rdd = key_value_duration_rdd.combineByKey(\n",
    "  (lambda x: (x, 1)), # wartość początkowa akumulatora (czas trwania interakcji, ilość elementów)\n",
    "  (lambda acc, value: (acc[0] + value, acc[1] + 1)),\n",
    "  (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))\n",
    ")\n",
    "\n",
    "# Zapisanie rezultatu w postaci mapy\n",
    "sum_counts_rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4a090d5b-cfc3-4287-a3b6-053a2d234264",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Posiadając dane w tej postaci możemy obliczyć średni czas trwania każdej interakcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dde8d023-9ee4-4157-93df-032a23ac7468",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "duration_means_by_type = sum_counts_rdd \\\n",
    "  .map(lambda row: (row[0], round(row[1][0]/row[1][1], 3))) \\\n",
    "  .collectAsMap()\n",
    "\n",
    "duration_means_by_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fb67dee5-6e77-43aa-ba11-d13f5ac074f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Dla przejrzystości przesortujmy jeszcze dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "30fd451b-9d47-46e0-b4f8-c9b0ef7c53f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tag in sorted(duration_means_by_type, key=duration_means_by_type.get, reverse=True):\n",
    "  print(\"{}: {}\".format(tag, duration_means_by_type[tag]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fdf99184-9fd9-4e40-95ef-319730d6aaf8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Ćwiczenia\n",
    "Efektem każdego podpunktu powinien być osobny obiekt RDD (który jest wejściem do kolejnego punktu).\n",
    "\n",
    "1. Należy zaimportować tablicę ze zmiennej `rawText` do RDD.\n",
    "2. Należy przetransformować wszystkie elementy do wielkich liter. Pomoc: [metoda `upper()`](https://docs.python.org/3/library/stdtypes.html#str.upper),  [transformacja `map()`](https://spark.apache.org/docs/latest/programming-guide.html#transformations)\n",
    "3. Należy podzielić poszczególny wiersz na poszczególne słowa tworząc z nich nowy RDD. Pomoc: [metoda `split()`](https://docs.python.org/3/library/stdtypes.html#str.split), [transformacja `flatMap()`](https://spark.apache.org/docs/latest/programming-guide.html#transformations)\n",
    "4. Należy odrzucić słowa zawierające mniej niż 3 znaki. Pomoc: [słowo kluczowe `len()`](https://docs.python.org/3/library/functions.html#len), [transformacja `filter()`](https://spark.apache.org/docs/latest/programming-guide.html#transformations)\n",
    "5. Należy utworzyć RDD typu klucz/wartość, gdzie kluczem jest pierwsza litera, a wartością długość słowa. Pomoc: [transformacja `map()`](https://spark.apache.org/docs/latest/programming-guide.html#transformations),\n",
    "6. Należy obliczyć średnią długość wyrazu rozpoczynającego się daną literą. Pomoc: [transformacja `combineByKey()`](https://spark.apache.org/docs/latest/programming-guide.html#transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8b7853e0-aebd-4a72-80cb-d9ccee0c5fb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_text = [\n",
    "  'All transformations in Spark are lazy in that they do not compute their results right away',\n",
    "  'Spark can create distributed datasets from any storage source supported by Hadoop',\n",
    "  'At a high level, every Spark application consists of a driver program that runs the users main function and executes various parallel operations on a cluster',\n",
    "  'The Shuffle is an expensive operation since it involves disk IO data serialization and network IO'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3a6d60d5-2059-4858-bd60-7e23e8460b7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# rdd_1 = ..."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "lab13",
   "notebookOrigID": 3334439263514720,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "name": "lab9",
  "notebookId": 4423948346929326
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
