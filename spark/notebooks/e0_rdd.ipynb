{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import findspark\n",
    "\n",
    "findspark.init(\n",
    "    spark_home=str(Path.cwd() / \"..\" / \"ext\" / \"spark-3.4.0-bin-hadoop3\")\n",
    ")\n",
    "\n",
    "from pyspark import SparkContext"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T20:08:22.279106Z",
     "start_time": "2023-06-14T20:08:22.217657Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating RDDs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/14 22:08:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default parallelism: 1\n",
      "Number of partitions: 1\n",
      "Partitioner: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions structure: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "nums = range(10)\n",
    "\n",
    "with SparkContext('local[1]') as sc:  # change me 1, 2, 15\n",
    "    rdd = sc.parallelize(nums) # change me\n",
    "\n",
    "    print(\"Default parallelism: {}\".format(sc.defaultParallelism))\n",
    "    print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "    print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "    print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T20:08:25.935013Z",
     "start_time": "2023-06-14T20:08:22.280644Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hurtownie', 'Danych', 'i', 'Big', 'Data', 'i', 'Coś', 'Jeszcze']\n",
      "[['Hurtownie', 'Danych', 'i', 'Big'], ['Data', 'i', 'Coś', 'Jeszcze']]\n"
     ]
    }
   ],
   "source": [
    "# from collection\n",
    "sentence = \"Hurtownie Danych i Big Data i Coś Jeszcze\"\n",
    "\n",
    "with SparkContext('local') as sc:\n",
    "    words = sc.parallelize(sentence.split(\" \"), 2)\n",
    "\n",
    "    print(words.collect())\n",
    "    print(words.glom().collect())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T20:08:27.429202Z",
     "start_time": "2023-06-14T20:08:25.934149Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Apache Spark\n"
     ]
    }
   ],
   "source": [
    "# from text file\n",
    "with SparkContext('local') as sc:\n",
    "    file_rdd = sc.textFile(\"../README.md\")\n",
    "    print(file_rdd.first())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T20:08:28.971103Z",
     "start_time": "2023-06-14T20:08:27.430001Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Manipulating RDDs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hurtownie', 'Danych', 'Big', 'Coś', 'Jeszcze', 'i', 'Data']\n"
     ]
    }
   ],
   "source": [
    "# duplicate removal\n",
    "with SparkContext('local') as sc:\n",
    "    words_rdd = sc.parallelize(sentence.split(\" \"), 2)\n",
    "    t1_rdd = words_rdd.distinct()\n",
    "    print(t1_rdd.collect())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T20:08:30.715128Z",
     "start_time": "2023-06-14T20:08:28.973845Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'i']\n"
     ]
    }
   ],
   "source": [
    "# filtering\n",
    "def starts_with(word, letter):\n",
    "  return word.startswith(letter)\n",
    "\n",
    "with SparkContext('local') as sc:\n",
    "    words_rdd = sc.parallelize(sentence.split(\" \"), 2)\n",
    "    t2_rdd = words_rdd.filter(lambda word: starts_with(word, 'i'))\n",
    "    print(t2_rdd.collect())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T20:08:32.159066Z",
     "start_time": "2023-06-14T20:08:30.717022Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hurtownie', 'H', False), ('Danych', 'D', False), ('i', 'i', False), ('Big', 'B', True), ('Data', 'D', False), ('i', 'i', False), ('Coś', 'C', False), ('Jeszcze', 'J', False)]\n"
     ]
    }
   ],
   "source": [
    "# map\n",
    "with SparkContext('local') as sc:\n",
    "    words_rdd = sc.parallelize(sentence.split(\" \"), 2)\n",
    "    t3_rdd = words_rdd.map(lambda word: (word, word[0], word.startswith(\"B\")))\n",
    "    print(t3_rdd.collect())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T20:08:33.610411Z",
     "start_time": "2023-06-14T20:08:32.161335Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hurtownie', 'Jeszcze', 'Danych']\n"
     ]
    }
   ],
   "source": [
    "# sort by desc word length\n",
    "with SparkContext('local') as sc:\n",
    "    words_rdd = sc.parallelize(sentence.split(\" \"), 2)\n",
    "    t4_rdd = words_rdd.sortBy(lambda word: len(word) * -1)\n",
    "    print(t4_rdd.take(3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T20:08:35.284263Z",
     "start_time": "2023-06-14T20:08:33.608260Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Danych', 'i', 'Big', 'Data', 'i', 'Coś', 'Jeszcze']\n",
      "['Hurtownie']\n"
     ]
    }
   ],
   "source": [
    "# random split with provided weights\n",
    "seed = 123\n",
    "\n",
    "with SparkContext('local') as sc:\n",
    "    words_rdd = sc.parallelize(sentence.split(\" \"), 2)\n",
    "\n",
    "    for rdd_split in words_rdd.randomSplit([0.8, 0.2], seed):\n",
    "        print(rdd_split.collect())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T20:08:36.797464Z",
     "start_time": "2023-06-14T20:08:35.282960Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hurtownie']\n"
     ]
    }
   ],
   "source": [
    "# sampling\n",
    "with SparkContext('local') as sc:\n",
    "    words_rdd = sc.parallelize(sentence.split(\" \"), 2)\n",
    "\n",
    "    t5_rdd = words_rdd.sample(False, .3)  # sample 30% without replacement\n",
    "    print(t5_rdd.collect())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T20:08:38.212633Z",
     "start_time": "2023-06-14T20:08:36.799389Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Actions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "with SparkContext('local') as sc:\n",
    "    t6_rdd = sc.parallelize(range(1, 21))\n",
    "    print(t6_rdd.reduce(lambda x, y: x + y))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T20:08:39.600739Z",
     "start_time": "2023-06-14T20:08:38.215859Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Hurtownie\n",
      "['Hurtownie', 'Danych']\n"
     ]
    }
   ],
   "source": [
    "with SparkContext('local') as sc:\n",
    "    words_rdd = sc.parallelize(sentence.split(\" \"), 2)\n",
    "    print(words_rdd.count())\n",
    "    print(words_rdd.first())\n",
    "    print(words_rdd.take(2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T20:08:41.116688Z",
     "start_time": "2023-06-14T20:08:39.601140Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "!rm -rf /tmp/words_rdd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T20:08:41.487926Z",
     "start_time": "2023-06-14T20:08:41.117187Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "with SparkContext('local') as sc:\n",
    "    words_rdd = sc.parallelize(sentence.split(\" \"), 2)\n",
    "    words_rdd.saveAsTextFile(\"file:/tmp/words_rdd\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T20:08:42.971984Z",
     "start_time": "2023-06-14T20:08:41.490005Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS    part-00000  part-00001\r\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/words_rdd/"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T20:08:43.354820Z",
     "start_time": "2023-06-14T20:08:42.974020Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;238m───────┬────────────────────────────────────────────────────────────────────────\u001B[0m\r\n",
      "       \u001B[38;5;238m│ \u001B[0mFile: \u001B[1m/tmp/words_rdd/part-00000\u001B[0m\r\n",
      "\u001B[38;5;238m───────┼────────────────────────────────────────────────────────────────────────\u001B[0m\r\n",
      "\u001B[38;5;238m   1\u001B[0m   \u001B[38;5;238m│\u001B[0m \u001B[38;5;238mHurtownie\u001B[0m\r\n",
      "\u001B[38;5;238m   2\u001B[0m   \u001B[38;5;238m│\u001B[0m \u001B[38;5;238mDanych\u001B[0m\r\n",
      "\u001B[38;5;238m   3\u001B[0m   \u001B[38;5;238m│\u001B[0m \u001B[38;5;238mi\u001B[0m\r\n",
      "\u001B[38;5;238m   4\u001B[0m   \u001B[38;5;238m│\u001B[0m \u001B[38;5;238mBig\u001B[0m\r\n",
      "\u001B[38;5;238m───────┴────────────────────────────────────────────────────────────────────────\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/words_rdd/part-00000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T20:08:43.728936Z",
     "start_time": "2023-06-14T20:08:43.355977Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Key-Value pairs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('H', 'hurtownie'), ('D', 'danych'), ('i', 'i'), ('B', 'big'), ('D', 'data'), ('i', 'i'), ('C', 'coś'), ('J', 'jeszcze')]\n",
      "{'H': 1, 'D': 2, 'i': 2, 'B': 1, 'C': 1, 'J': 1}\n",
      "[('H', 'HURTOWNIE'), ('D', 'DANYCH'), ('i', 'I'), ('B', 'BIG'), ('D', 'DATA'), ('i', 'I'), ('C', 'COŚ'), ('J', 'JESZCZE')]\n"
     ]
    }
   ],
   "source": [
    "with SparkContext('local') as sc:\n",
    "    words_rdd = sc.parallelize(sentence.split(\" \"), 2)\n",
    "    t7_rdd = words_rdd.map(lambda word: (word[0], word.lower()))\n",
    "\n",
    "    print(t7_rdd.collect())\n",
    "    print(dict(t7_rdd.countByKey()))\n",
    "    print(t7_rdd.mapValues(lambda word: word.upper()).collect())\n",
    "    # print(t7_rdd.groupByKey().collect())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T20:08:45.335822Z",
     "start_time": "2023-06-14T20:08:43.731893Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
